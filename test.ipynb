{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'e:\\\\Python\\\\Lib\\\\site-packages\\\\dash_html_components-2.0.0.dist-info\\\\entry_points.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\matplotlib\\__init__.py:1283\u001b[0m\n\u001b[0;32m   1279\u001b[0m     rcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbackend_fallback\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1282\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m-> 1283\u001b[0m     \u001b[43mrcParams\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbackend\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMPLBACKEND\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_backend\u001b[39m():\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m \u001b[38;5;124;03m    Return the name of the current backend.\u001b[39;00m\n\u001b[0;32m   1289\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1292\u001b[0m \u001b[38;5;124;03m    matplotlib.use\u001b[39;00m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\matplotlib\\__init__.py:749\u001b[0m, in \u001b[0;36mRcParams.__setitem__\u001b[1;34m(self, key, val)\u001b[0m\n\u001b[0;32m    747\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    748\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 749\u001b[0m     cval \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    750\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ve:\n\u001b[0;32m    751\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mKey \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mve\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\matplotlib\\rcsetup.py:273\u001b[0m, in \u001b[0;36mvalidate_backend\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mvalidate_backend\u001b[39m(s):\n\u001b[1;32m--> 273\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m _auto_backend_sentinel \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mbackend_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_valid_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    274\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[0;32m    275\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\matplotlib\\backends\\registry.py:243\u001b[0m, in \u001b[0;36mBackendRegistry.is_valid_backend\u001b[1;34m(self, backend)\u001b[0m\n\u001b[0;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;66;03m# Only load entry points if really need to and not already done so.\u001b[39;00m\n\u001b[1;32m--> 243\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_entry_points_loaded\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m backend \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend_to_gui_framework:\n\u001b[0;32m    245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\matplotlib\\backends\\registry.py:113\u001b[0m, in \u001b[0;36mBackendRegistry._ensure_entry_points_loaded\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_ensure_entry_points_loaded\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;66;03m# Load entry points, if they have not already been loaded.\u001b[39;00m\n\u001b[0;32m    112\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_entry_points:\n\u001b[1;32m--> 113\u001b[0m         entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_read_entry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_and_store_entry_points(entries)\n\u001b[0;32m    115\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loaded_entry_points \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32me:\\Python\\Lib\\site-packages\\matplotlib\\backends\\registry.py:137\u001b[0m, in \u001b[0;36mBackendRegistry._read_entry_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    135\u001b[0m group \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmatplotlib.backend\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m sys\u001b[38;5;241m.\u001b[39mversion_info \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m10\u001b[39m):\n\u001b[1;32m--> 137\u001b[0m     entry_points \u001b[38;5;241m=\u001b[39m \u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m     entry_points \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mentry_points()\u001b[38;5;241m.\u001b[39mget(group, ())\n",
      "File \u001b[1;32me:\\Python\\Lib\\importlib\\metadata\\__init__.py:1040\u001b[0m, in \u001b[0;36mentry_points\u001b[1;34m(**params)\u001b[0m\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03mPass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m:return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[0;32m   1038\u001b[0m     dist\u001b[38;5;241m.\u001b[39mentry_points \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1039\u001b[0m )\n\u001b[1;32m-> 1040\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSelectableGroups\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32me:\\Python\\Lib\\importlib\\metadata\\__init__.py:476\u001b[0m, in \u001b[0;36mSelectableGroups.load\u001b[1;34m(cls, eps)\u001b[0m\n\u001b[0;32m    473\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload\u001b[39m(\u001b[38;5;28mcls\u001b[39m, eps):\n\u001b[0;32m    475\u001b[0m     by_group \u001b[38;5;241m=\u001b[39m operator\u001b[38;5;241m.\u001b[39mattrgetter(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgroup\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 476\u001b[0m     ordered \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msorted\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    477\u001b[0m     grouped \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mgroupby(ordered, by_group)\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m((group, EntryPoints(eps)) \u001b[38;5;28;01mfor\u001b[39;00m group, eps \u001b[38;5;129;01min\u001b[39;00m grouped)\n",
      "File \u001b[1;32me:\\Python\\Lib\\importlib\\metadata\\__init__.py:1038\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[EntryPoints, SelectableGroups]:\n\u001b[0;32m   1021\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return EntryPoint objects for all installed packages.\u001b[39;00m\n\u001b[0;32m   1022\u001b[0m \n\u001b[0;32m   1023\u001b[0m \u001b[38;5;124;03m    Pass selection parameters (group or name) to filter the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;124;03m    :return: EntryPoints or SelectableGroups for all installed packages.\u001b[39;00m\n\u001b[0;32m   1036\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   1037\u001b[0m     eps \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m-> 1038\u001b[0m         \u001b[43mdist\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mentry_points\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m dist \u001b[38;5;129;01min\u001b[39;00m _unique(distributions())\n\u001b[0;32m   1039\u001b[0m     )\n\u001b[0;32m   1040\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SelectableGroups\u001b[38;5;241m.\u001b[39mload(eps)\u001b[38;5;241m.\u001b[39mselect(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n",
      "File \u001b[1;32me:\\Python\\Lib\\importlib\\metadata\\__init__.py:636\u001b[0m, in \u001b[0;36mDistribution.entry_points\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    634\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    635\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mentry_points\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 636\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m EntryPoints\u001b[38;5;241m.\u001b[39m_from_text_for(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mentry_points.txt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[1;32me:\\Python\\Lib\\importlib\\metadata\\__init__.py:938\u001b[0m, in \u001b[0;36mPathDistribution.read_text\u001b[1;34m(self, filename)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, filename):\n\u001b[0;32m    931\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m suppress(\n\u001b[0;32m    932\u001b[0m         \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m,\n\u001b[0;32m    933\u001b[0m         \u001b[38;5;167;01mIsADirectoryError\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    936\u001b[0m         \u001b[38;5;167;01mPermissionError\u001b[39;00m,\n\u001b[0;32m    937\u001b[0m     ):\n\u001b[1;32m--> 938\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_path\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoinpath\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\Python\\Lib\\pathlib.py:1058\u001b[0m, in \u001b[0;36mPath.read_text\u001b[1;34m(self, encoding, errors)\u001b[0m\n\u001b[0;32m   1054\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1055\u001b[0m \u001b[38;5;124;03mOpen the file in text mode, read it, and close the file.\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1057\u001b[0m encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1058\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m   1059\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f\u001b[38;5;241m.\u001b[39mread()\n",
      "File \u001b[1;32me:\\Python\\Lib\\pathlib.py:1044\u001b[0m, in \u001b[0;36mPath.open\u001b[1;34m(self, mode, buffering, encoding, errors, newline)\u001b[0m\n\u001b[0;32m   1042\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1043\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mtext_encoding(encoding)\n\u001b[1;32m-> 1044\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m io\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m, mode, buffering, encoding, errors, newline)\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'e:\\\\Python\\\\Lib\\\\site-packages\\\\dash_html_components-2.0.0.dist-info\\\\entry_points.txt'"
     ]
    }
   ],
   "source": [
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Set matplotlib backend to Agg\n",
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "\n",
    "# Function to filter buy signals from the 1-minute chart data\n",
    "def filter_signals(data):\n",
    "    if 'Buy Signal SOSI Final' not in data.columns:\n",
    "        print(\"Warning: 'Buy Signal SOSI Final' column not found in the data.\")\n",
    "        if 'volume' not in data.columns and 'Volume' not in data.columns:\n",
    "            print(\"Error: 'volume' column not found in the data.\")\n",
    "            return pd.DataFrame()\n",
    "        data = calculate_indicators(data)\n",
    "        if 'Buy Signal SOSI Final' not in data.columns:\n",
    "            print(\"Error: 'Buy Signal SOSI Final' column still not found in the data.\")\n",
    "            return pd.DataFrame()\n",
    "        buy_signals = data[(data['Buy Signal SOSI Final'] == True)]\n",
    "        return buy_signals\n",
    "\n",
    "    buy_signals = data[(data['Buy Signal SOSI Final'] == True)]\n",
    "    return buy_signals\n",
    "\n",
    "# Function to calculate indicators and buy signals\n",
    "def calculate_indicators(data):\n",
    "    # Calculate VWPCI\n",
    "    if 'volume' in data.columns:\n",
    "        volume = data['volume']\n",
    "    elif 'Volume' in data.columns:\n",
    "        volume = data['Volume']\n",
    "    data['priceChangeRatio'] = (data['close'] - data['open']) / (data['high'] - data['low'])\n",
    "    data['VWPCI'] = data['priceChangeRatio'] * volume\n",
    "\n",
    "    # Convert Unix timestamp (UTC) to datetime\n",
    "    data['time_utc'] = pd.to_datetime(data['time'], unit='s', utc=True)\n",
    "\n",
    "    # Convert UTC to IST\n",
    "    data['time_ist'] = data['time_utc'] + timedelta(hours=5, minutes=30)\n",
    "\n",
    "    # Calculate ROC\n",
    "    roc_periods = 14\n",
    "    data['ROC'] = data['close'].pct_change(periods=roc_periods)\n",
    "\n",
    "    # Calculate MVI\n",
    "    data['MVI'] = volume * ((data['close'] - data['open']) / data['close'])\n",
    "\n",
    "    # Calculate VOLD\n",
    "    data['up_volume'] = np.where(data['close'] > data['close'].shift(1), volume, 0)\n",
    "    data['down_volume'] = np.where(data['close'] <= data['close'].shift(1), volume, 0)\n",
    "    data['total_up_volume'] = data['up_volume'].cumsum()\n",
    "    data['total_down_volume'] = data['down_volume'].cumsum()\n",
    "    data['VOLD_ratio'] = data['total_up_volume'] / data['total_down_volume']\n",
    "\n",
    "    # Normalize VOLD\n",
    "    data['normalized_vold'] = (data['VOLD_ratio'] - data['VOLD_ratio'].rolling(window=100).min()) / \\\n",
    "                              (data['VOLD_ratio'].rolling(window=100).max() - data['VOLD_ratio'].rolling(window=100).min())\n",
    "\n",
    "    # Theta Calculation\n",
    "    days_to_expiration = 30  # assuming a constant value\n",
    "    d1 = (np.log(data['close'] / data['close']) + (0.05 + (0.2 * 0.2) / 2) * days_to_expiration / 365) / (0.2 * np.sqrt(days_to_expiration / 365))\n",
    "    d2 = d1 - 0.2 * np.sqrt(days_to_expiration / 365)\n",
    "    theta_call = -(data['close'] * 0.2 * np.exp(-0.05 * days_to_expiration / 365) * (np.exp(-(d1 ** 2) / 2) / np.sqrt(2 * np.pi))) / \\\n",
    "                 (2 * np.sqrt(days_to_expiration / 365)) - 0.05 * 100 * np.exp(-0.05 * days_to_expiration / 365) * \\\n",
    "                 (np.exp(-(d2 ** 2) / 2) / np.sqrt(2 * np.pi))\n",
    "    data['theta'] = theta_call\n",
    "\n",
    "    # Normalize Theta\n",
    "    data['normalized_theta'] = (data['theta'] - data['theta'].rolling(window=100).min()) / \\\n",
    "                               (data['theta'].rolling(window=100).max() - data['theta'].rolling(window=100).min())\n",
    "\n",
    "    # Composite SOSI Calculation\n",
    "    data['sosi_base'] = 0.1 * data['VWPCI'] + 0.2 * data['ROC'] + 0.3 * data['MVI']\n",
    "    data['sosi'] = data['sosi_base'] + 0.4 * data['normalized_vold'] + 0.1 * data['normalized_theta']\n",
    "\n",
    "    # Calculate slopes\n",
    "    data['sosi_slope'] = data['sosi'] - data['sosi'].shift(1)\n",
    "    data['vold_slope'] = data['normalized_vold'] - data['normalized_vold'].shift(1)\n",
    "\n",
    "    # Define conditions\n",
    "    data['condition1'] = (data['sosi'] < 0) & (data['sosi_slope'] > 0) & (data['normalized_vold'] < 0) & (data['vold_slope'] > 0) & (data['sosi'] < data['normalized_vold'])\n",
    "    data['condition2'] = (data['sosi'] > 0) & (data['sosi_slope'] > 0) & (data['normalized_vold'] > 0) & (data['vold_slope'] > 0) & (data['sosi'] > data['normalized_vold'])\n",
    "    data['condition3'] = (data['sosi'] < 0) & (data['sosi_slope'] > 0) & (data['sosi'].shift(1) < data['sosi'].shift(2)) & (data['normalized_vold'] < 0) & (data['vold_slope'] > 0) & (data['normalized_vold'].shift(1) < data['normalized_vold'].shift(2))\n",
    "    data['condition4'] = (data['sosi_slope'] > 0) & (data['vold_slope'] > 0) & (data['sosi'] < data['normalized_vold'])\n",
    "\n",
    "    # Combine conditions into the buy signal\n",
    "    data['Buy Signal SOSI Final'] = (data['time_ist'].dt.time == pd.Timestamp('09:20:00').time()) & (data['condition1'] | data['condition2'] | data['condition3'] | data['condition4'])\n",
    "\n",
    "    return data\n",
    "\n",
    "# Function to calculate charges\n",
    "def calculate_charges(entry_price, exit_price, quantity):\n",
    "    brokerage = 40  # total brokerage for one complete buy and sell\n",
    "    stt_ctt = 0.00125 * exit_price * quantity\n",
    "    transaction_charges = 0.000495 * (entry_price + exit_price) * quantity\n",
    "    gst = 0.18 * (brokerage + transaction_charges)\n",
    "    sebi_charges = 10 / 10**7 * (entry_price + exit_price) * quantity\n",
    "    stamp_charges = 0.00003 * entry_price * quantity\n",
    "    total_charges = brokerage + stt_ctt + transaction_charges + gst + sebi_charges + stamp_charges\n",
    "    return total_charges\n",
    "\n",
    "# Function to simulate day trades with error margin\n",
    "def simulate_day_trades(buy_signals, minute_data, symbol, profit_target=0.02, stop_loss=0.01, min_stop_loss=0.005, error_margin=0.03):\n",
    "    results = []\n",
    "    capital = 100000  # starting capital\n",
    "\n",
    "    for index, buy_signal in buy_signals.iterrows():\n",
    "        entry_price = buy_signal['close'] * (1 + error_margin)  # Adjust entry price with error margin\n",
    "        entry_time = buy_signal['time'] + 60  # Add 1 minute to the buy signal time to get the entry time\n",
    "\n",
    "        # Convert entry_time from epoch to datetime\n",
    "        entry_datetime = pd.to_datetime(entry_time, unit='s')\n",
    "        entry_datetime_ist = entry_datetime + timedelta(hours=5, minutes=30)  # Convert to IST\n",
    "        \n",
    "        # Check if the entry time is post 2:30 PM IST\n",
    "        if entry_datetime_ist.time() >= datetime.strptime('14:30:00', '%H:%M:%S').time():\n",
    "            continue  # Skip trades initiated after 2:30 PM IST\n",
    "\n",
    "        profit_price = entry_price * (1 + profit_target)\n",
    "        stop_price = entry_price * (1 - max(stop_loss, min_stop_loss))  # Ensure stop loss is at least min_stop_loss\n",
    "        \n",
    "        trade_result = {\n",
    "            'signal_time': buy_signal['time'],\n",
    "            'entry_time': entry_time,\n",
    "            'entry_price': entry_price,\n",
    "            'Target Profit': profit_target,\n",
    "            'Stop Loss': stop_loss,\n",
    "            'Profit Price': profit_price,\n",
    "            'Stop Price': stop_price,\n",
    "            'exit_time': None,\n",
    "            'exit_price': None,\n",
    "            'charges': None,\n",
    "            'profit_before_charges': None,\n",
    "            'profit': None,\n",
    "            'exit_reason': None,\n",
    "            'SOSI': buy_signal.get('SOSI'),\n",
    "            'Normalized VOLD Ratio': buy_signal.get('Normalized VOLD Ratio'),\n",
    "            'Normalized Theta': buy_signal.get('Normalized Theta'),\n",
    "            'capital_before_trade': capital\n",
    "        }\n",
    "        \n",
    "        # Filter subsequent data for the same day only\n",
    "        trade_date = pd.to_datetime(entry_time, unit='s').date()\n",
    "        subsequent_data = minute_data[(minute_data['time'] > entry_time) & \n",
    "                                      (pd.to_datetime(minute_data['time'], unit='s').dt.date == trade_date)]\n",
    "        \n",
    "        for _, row in subsequent_data.iterrows():\n",
    "            if row['high'] >= profit_price:\n",
    "                trade_result['exit_time'] = row['time']\n",
    "                trade_result['exit_price'] = profit_price\n",
    "                trade_result['exit_reason'] = 'Target Hit'\n",
    "                break\n",
    "            if row['low'] <= stop_price:\n",
    "                trade_result['exit_time'] = row['time']\n",
    "                trade_result['exit_price'] = stop_price\n",
    "                trade_result['exit_reason'] = 'Stop Loss Hit'\n",
    "                break\n",
    "        else:\n",
    "            # If no target or stop loss hit, exit at the last price of the day\n",
    "            trade_result['exit_reason'] = 'End of Day Exit'\n",
    "            if not subsequent_data.empty:\n",
    "                last_row = subsequent_data.iloc[-1]\n",
    "                trade_result['exit_time'] = last_row['time']\n",
    "                trade_result['exit_price'] = last_row['close']\n",
    "        \n",
    "        if trade_result['exit_price'] is not None:\n",
    "            quantity = 25 if 'NIFTY' in symbol else 15 if 'BANKNIFTY' in symbol else 40 if 'FINNIFTY' in symbol else 10\n",
    "            gross_profit = (trade_result['exit_price'] - trade_result['entry_price']) * quantity\n",
    "            charges = calculate_charges(trade_result['entry_price'], trade_result['exit_price'], quantity)\n",
    "            net_profit = gross_profit - charges\n",
    "            trade_result['charges'] = charges\n",
    "            trade_result['profit_before_charges'] = gross_profit\n",
    "            trade_result['profit'] = net_profit\n",
    "            capital += net_profit  # Update the capital after each trade\n",
    "            trade_result['capital_after_trade'] = capital\n",
    "\n",
    "        results.append(trade_result)\n",
    "    \n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    if results_df.empty:\n",
    "        return pd.DataFrame(columns=['signal_time', 'entry_time', 'entry_price', 'Target Profit', 'Stop Loss', \n",
    "                                     'Profit Price', 'Stop Price', 'exit_time', 'exit_price', 'charges', \n",
    "                                     'profit_before_charges', 'profit', 'exit_reason', 'SOSI', \n",
    "                                     'Normalized VOLD Ratio', 'Normalized Theta', 'capital_before_trade', 'capital_after_trade'])\n",
    "    \n",
    "    return results_df.dropna(subset=['entry_price'])\n",
    "\n",
    "# Function to analyze profit and stop loss percentages\n",
    "def analyze_profit_and_stop_loss(buy_signals, minute_data, profit_percentages, stop_loss_percentages, min_stop_loss=0.005, error_margin=0.01):\n",
    "    analysis_results = []\n",
    "    finalresults = []\n",
    "    for profit_target in profit_percentages:\n",
    "        for stop_loss in stop_loss_percentages:\n",
    "            trade_results = simulate_day_trades(buy_signals, minute_data, folder_name, profit_target, stop_loss, min_stop_loss, error_margin)\n",
    "            total_profit = trade_results['profit'].sum()\n",
    "            charges = trade_results['charges'].sum()\n",
    "            profit_before_charges = trade_results['profit_before_charges'].sum()\n",
    "            analysis_results.append((profit_target, stop_loss, charges, profit_before_charges, total_profit))\n",
    "            finalresults.append(trade_results)\n",
    "    # Combine all trade results into a single DataFrame\n",
    "    combined_trade_results = pd.concat(finalresults, ignore_index=True)\n",
    " \n",
    "    return pd.DataFrame(analysis_results, columns=['Profit Target', 'Stop Loss', 'Charges', 'Profit Before Charges', 'Total Profit']), combined_trade_results\n",
    "\n",
    "# Function to summarize trade results\n",
    "def summarize_trade_results(trade_results):\n",
    "    total_trades = len(trade_results)\n",
    "    target_hit = len(trade_results[trade_results['exit_reason'] == 'Target Hit'])\n",
    "    stop_loss_hit = len(trade_results[trade_results['exit_reason'] == 'Stop Loss Hit'])\n",
    "    end_of_day_exit = len(trade_results[trade_results['exit_reason'] == 'End of Day Exit'])\n",
    "    max_drawdown = trade_results['profit'].min()\n",
    "    summary = {\n",
    "        'Total Trades': total_trades,\n",
    "        'Target Hit': target_hit,\n",
    "        'Stop Loss Hit': stop_loss_hit,\n",
    "        'End of Day Exit': end_of_day_exit,\n",
    "        'Max Drawdown': max_drawdown\n",
    "    }\n",
    "    return summary\n",
    "\n",
    "# Define profit percentages and stop loss percentages to analyze\n",
    "profit_percentages = [i / 100 for i in range(1, 2)]\n",
    "stop_loss_percentages = [i / 100 for i in range(1, 2)]\n",
    "\n",
    "# Directories containing the data\n",
    "directories = ['June 2024/NIFTY', 'June 2024/BANKNIFTY', 'June 2024/FINNIFTY']  # Change the directories as needed\n",
    "\n",
    "# Function to check if the date is within the week of expiration\n",
    "def is_within_expiry_week(expiry_date, timestamp, folder_name, days_before_expiry=5):\n",
    "    # Assuming the date is the 7th to 12th characters in the string\n",
    "    if folder_name == 'NIFTY':\n",
    "        date_part = expiry_date[9:15]\n",
    "    if folder_name == 'BANKNIFTY':\n",
    "        date_part = expiry_date[13:19]\n",
    "    if folder_name == 'FINNIFTY':\n",
    "        date_part = expiry_date[12:18]\n",
    "    if folder_name == 'SENSEX':\n",
    "        date_part = expiry_date[7:13]\n",
    "    \n",
    "    expiry_datetime = pd.to_datetime(date_part, format='%y%m%d')\n",
    "    signal_datetime = pd.to_datetime(timestamp, unit='s')\n",
    "    return (expiry_datetime - signal_datetime).days <= days_before_expiry\n",
    "\n",
    "# Process each directory\n",
    "for directory in directories:\n",
    "    pnl_results = []\n",
    "    trade_results_combined = []\n",
    "    all_combined_results = []\n",
    "    folder_name = os.path.basename(directory)\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        if subdir == directory:\n",
    "            continue  # Skip the top-level directory itself\n",
    "        for filename in files:\n",
    "            if ', 1.csv' in filename:  # Change to ', 1.csv' for 1-minute data\n",
    "                minute_1_path = os.path.join(subdir, filename)\n",
    "                \n",
    "                if os.path.exists(minute_1_path):\n",
    "                    print(f\"Processing {filename} in {folder_name}...\")\n",
    "                    \n",
    "                    # Extract expiry date from filename\n",
    "                    expiry_date = filename.split(' ')[0].split(',')[0]\n",
    "                    \n",
    "                    # Load the CSV file\n",
    "                    minute_data_1 = pd.read_csv(minute_1_path)\n",
    "                    \n",
    "                    # Filter buy signals\n",
    "                    buy_signals = filter_signals(minute_data_1)\n",
    "\n",
    "                    # Skip if no buy signals\n",
    "                    if buy_signals.empty:\n",
    "                        print(f\"No buy signals found for {filename}. Skipping...\")\n",
    "                        continue\n",
    "                    # Filter signals to only include those within the expiry week\n",
    "                    buy_signals = buy_signals[buy_signals['time'].apply(lambda x: is_within_expiry_week(expiry_date, x, folder_name))]\n",
    "                    \n",
    "                    # Skip if no buy signals within expiry week\n",
    "                    if buy_signals.empty:\n",
    "                        print(f\"No buy signals found within expiry week for {filename}. Skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Perform the analysis\n",
    "                    analysis_results, trade_results = analyze_profit_and_stop_loss(buy_signals, minute_data_1, profit_percentages, stop_loss_percentages)\n",
    "                    \n",
    "                    # Merge buy signals with trade results\n",
    "                    combined_results = pd.merge(buy_signals, trade_results, left_on='time', right_on='signal_time', how='inner')\n",
    "                    \n",
    "                    # Merge the 1-minute chart data with combined results\n",
    "                    combined_results = pd.merge(combined_results, minute_data_1, left_on='time', right_on='time', suffixes=('_buy', '_original'))\n",
    "                   \n",
    "                    all_combined_results.append(combined_results)\n",
    "                    \n",
    "                    # Append to combined results\n",
    "                    pnl_results.append(analysis_results)\n",
    "                    trade_results_combined.append(trade_results)\n",
    "    \n",
    "    # Combine all results for the folder\n",
    "    if not pnl_results:\n",
    "        print(f\"No data found for {folder_name}. Skipping...\")\n",
    "        continue    \n",
    "    combined_pnl_results = pd.concat(pnl_results, ignore_index=True)\n",
    "    # Aggregate total profits for the same combinations\n",
    "    combined_pnl_results = combined_pnl_results.groupby(['Profit Target', 'Stop Loss'], as_index=False)['Total Profit'].sum()\n",
    "    combined_trade_results = pd.concat(trade_results_combined, ignore_index=True)\n",
    "    combined_all_results = pd.concat(all_combined_results, ignore_index=True)\n",
    "    \n",
    "    # Save the results to CSV files\n",
    "    output_dir = f'Outputs/DayTrade/SOSI/1min/{folder_name}'\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    combined_pnl_results.to_csv(f'{output_dir}/{folder_name}_pnl_analysis.csv', index=False)\n",
    "    combined_trade_results.to_csv(f'{output_dir}/{folder_name}_trade_results.csv', index=False)\n",
    "    combined_all_results.to_csv(f'{output_dir}/{folder_name}_all_combined_results.csv', index=False)\n",
    "    \n",
    "    # Find the best combination of profit target and stop loss\n",
    "    if not combined_pnl_results.empty:\n",
    "        best_combination = combined_pnl_results.loc[combined_pnl_results['Total Profit'].idxmax()]\n",
    "        \n",
    "        # Save the best combination\n",
    "        best_combination_df = pd.DataFrame([best_combination])\n",
    "        best_combination_df.to_csv(f'{output_dir}/{folder_name}_best_combination        .csv', index=False)\n",
    "        \n",
    "        # Save the top 40 combinations\n",
    "        top_40_combinations = combined_pnl_results.sort_values(by='Total Profit', ascending=False).head(40)\n",
    "        top_40_combinations.to_csv(f'{output_dir}/{folder_name}_top_40_combinations.csv', index=False)\n",
    "        \n",
    "        # Filter the best trade results from the combined trade results\n",
    "        best_trade_results = combined_trade_results[(combined_trade_results['Target Profit'] == best_combination['Profit Target']) & \n",
    "                                                    (combined_trade_results['Stop Loss'] == best_combination['Stop Loss'])]\n",
    "        \n",
    "        # Summarize the trade results for the best combination\n",
    "        trade_summary = summarize_trade_results(best_trade_results)\n",
    "        \n",
    "        # Save the trade summary\n",
    "        trade_summary_df = pd.DataFrame([trade_summary])\n",
    "        trade_summary_df.to_csv(f'{output_dir}/{folder_name}_trade_summary.csv', index=False)\n",
    "        \n",
    "        # Plot the pie chart for trade summary\n",
    "        labels = ['Target Hit', 'Stop Loss Hit', 'End of Day Exit']\n",
    "        sizes = [trade_summary['Target Hit'], trade_summary['Stop Loss Hit'], trade_summary['End of Day Exit']]\n",
    "        colors = ['lightgreen', 'lightcoral', 'lightskyblue']\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "        plt.title(f'Trade Summary for Best PNL Combination\\n{folder_name}')\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "        plt.savefig(f'{output_dir}/{folder_name}_trade_summary_pie_chart.png')\n",
    "        plt.close()\n",
    "        \n",
    "        # Plot the bar chart for total trades\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(['Total Trades'], [trade_summary['Total Trades']], color='skyblue')\n",
    "        plt.xlabel('Count')\n",
    "        plt.ylabel('Total Trades')\n",
    "        plt.title(f'Total Trades for Best PNL Combination\\n{folder_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(f'{output_dir}/{folder_name}_total_trades_bar_chart.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot the bar chart for max drawdown\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(['Max Drawdown'], [trade_summary['Max Drawdown']], color='red')\n",
    "        plt.xlabel('Max Drawdown')\n",
    "        plt.ylabel('Max Drawdown Value')\n",
    "        plt.title(f'Max Drawdown for Best PNL Combination\\n{folder_name}')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(f'{output_dir}/{folder_name}_max_drawdown_bar_chart.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot the heatmap for better visualization\n",
    "        pivot_table = combined_pnl_results.pivot(index=\"Stop Loss\", columns=\"Profit Target\", values=\"Total Profit\")\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Total Profit'})\n",
    "        plt.title(f\"Total Profit for Different Profit Targets and Stop Loss Percentages\\n{folder_name}\")\n",
    "        plt.xlabel(\"Profit Target\")\n",
    "        plt.ylabel(\"Stop Loss\")\n",
    "        plt.show()\n",
    "        plt.savefig(f'{output_dir}/{folder_name}_heatmap.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Plot the bar chart for the top 40 combinations\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.barh(top_40_combinations['Profit Target'].astype(str) + \" / \" + top_40_combinations['Stop Loss'].astype(str),\n",
    "                 top_40_combinations['Total Profit'], color='skyblue')\n",
    "        plt.xlabel('Total Profit')\n",
    "        plt.ylabel('Profit Target / Stop Loss')\n",
    "        plt.title(f'Top 40 Profit Target and Stop Loss Combinations\\n{folder_name}')\n",
    "        plt.gca().invert_yaxis()  # To display the highest\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        plt.savefig(f'{output_dir}/{folder_name}_top_40_combinations.png')\n",
    "        plt.close()\n",
    "\n",
    "        # Ensure all indicators are included in the calculations, even if some have no data\n",
    "        def combined_range_analysis():\n",
    "            indicators = ['SOSI', 'Normalized VOLD Ratio', 'Normalized Theta']\n",
    "            stop_loss_hits = combined_trade_results[combined_trade_results['exit_reason'] == 'Stop Loss Hit']\n",
    "            target_hits = combined_trade_results[combined_trade_results['exit_reason'] == 'Target Hit']\n",
    "            end_of_day_exits = combined_trade_results[combined_trade_results['exit_reason'] == 'End of Day Exit']\n",
    "        \n",
    "            if stop_loss_hits.empty and target_hits.empty and end_of_day_exits.empty:\n",
    "                print(\"No data available for combined range analysis.\")\n",
    "                return\n",
    "        \n",
    "            def get_min_max(data, indicator):\n",
    "                if indicator in data.columns:\n",
    "                    return data[indicator].min(), data[indicator].max()\n",
    "                else:\n",
    "                    return float('nan'), float('nan')\n",
    "        \n",
    "            stop_loss_ranges = pd.Series({indicator: get_min_max(stop_loss_hits, indicator) for indicator in indicators})\n",
    "            target_hit_ranges = pd.Series({indicator: get_min_max(target_hits, indicator) for indicator in indicators})\n",
    "            end_of_day_exit_ranges = pd.Series({indicator: get_min_max(end_of_day_exits, indicator) for indicator in indicators})\n",
    "        \n",
    "            combined_data = {\n",
    "                'Indicator': indicators,\n",
    "                'Stop Loss Min': [rng[0] for rng in stop_loss_ranges],\n",
    "                'Stop Loss Max': [rng[1] for rng in stop_loss_ranges],\n",
    "                'Target Hit Min': [rng[0] for rng in target_hit_ranges],\n",
    "                'Target Hit Max': [rng[1] for rng in target_hit_ranges],\n",
    "                'End of Day Exit Min': [rng[0] for rng in end_of_day_exit_ranges],\n",
    "                'End of Day Exit Max': [rng[1] for rng in end_of_day_exit_ranges]\n",
    "            }\n",
    "        \n",
    "            combined_data_df = pd.DataFrame(combined_data)\n",
    "        \n",
    "            # Convert all relevant columns to numeric\n",
    "            for col in ['Stop Loss Min', 'Stop Loss Max', 'Target Hit Min', 'Target Hit Max', 'End of Day Exit Min', 'End of Day Exit Max']:\n",
    "                combined_data_df[col] = pd.to_numeric(combined_data_df[col], errors='coerce')\n",
    "        \n",
    "            # Ensure there is numeric data to plot\n",
    "            numeric_data = combined_data_df.drop(columns=['Indicator']).dropna(how='all', axis=1)\n",
    "        \n",
    "            if not numeric_data.empty and not combined_data_df.empty:\n",
    "                combined_data_df.set_index('Indicator').plot(kind='bar', figsize=(12, 8))\n",
    "                plt.title(f'Combined Range Analysis for Best PNL Combination\\n{folder_name}')\n",
    "                plt.ylabel('Range')\n",
    "                plt.show()\n",
    "                plt.savefig(f'{output_dir}/{folder_name}_combined_range_analysis.png')\n",
    "                plt.close()\n",
    "            else:\n",
    "                print(\"No numeric data available for combined range analysis.\")\n",
    "        \n",
    "\n",
    "        # Call the function to perform the combined range analysis\n",
    "        combined_range_analysis()\n",
    "\n",
    "        def identify_ranges_to_avoid():\n",
    "            indicators = ['SOSI', 'Normalized VOLD Ratio', 'Normalized Theta']\n",
    "            stop_loss_hits = combined_trade_results[combined_trade_results['exit_reason'] == 'Stop Loss Hit']\n",
    "            target_hits = combined_trade_results[combined_trade_results['exit_reason'] == 'Target Hit']\n",
    "        \n",
    "            def get_min_max(data, indicator):\n",
    "                if indicator in data.columns:\n",
    "                    return data[indicator].min(), data[indicator].max()\n",
    "                else:\n",
    "                    return float('nan'), float('nan')\n",
    "        \n",
    "            stop_loss_ranges = {indicator: get_min_max(stop_loss_hits, indicator) for indicator in indicators}\n",
    "            target_hit_ranges = {indicator: get_min_max(target_hits, indicator) for indicator in indicators}\n",
    "            \n",
    "            combined_data = {\n",
    "                'Indicator': indicators,\n",
    "                'Stop Loss Min': [stop_loss_ranges[ind][0] for ind in indicators],\n",
    "                'Stop Loss Max': [stop_loss_ranges[ind][1] for ind in indicators],\n",
    "                'Target Hit Min': [target_hit_ranges[ind][0] for ind in indicators],\n",
    "                'Target Hit Max': [target_hit_ranges[ind][1] for ind in indicators],\n",
    "            }\n",
    "        \n",
    "            combined_data_df = pd.DataFrame(combined_data)\n",
    "        \n",
    "            # Convert all relevant columns to numeric\n",
    "            for col in ['Stop Loss Min', 'Stop Loss Max', 'Target Hit Min', 'Target Hit Max']:\n",
    "                combined_data_df[col] = pd.to_numeric(combined_data_df[col], errors='coerce')\n",
    "        \n",
    "            # Ensure there is numeric data to plot\n",
    "            numeric_data = combined_data_df.drop(columns=['Indicator']).dropna(how='all', axis=1)\n",
    "        \n",
    "            if not numeric_data.empty:\n",
    "                combined_data_df.set_index('Indicator').plot(kind='bar', figsize=(12, 8))\n",
    "                plt.title(f'Combined Range Analysis for Best PNL Combination\\n{folder_name}')\n",
    "                plt.ylabel('Range')\n",
    "                plt.show()\n",
    "                plt.savefig(f'{output_dir}/{folder_name}_combined_range_analysis.png')\n",
    "                plt.close()\n",
    "            else:\n",
    "                print(\"No numeric data available for combined range analysis.\")\n",
    "        \n",
    "            # Print the ranges to avoid\n",
    "            ranges_to_avoid = {}\n",
    "            for indicator in indicators:\n",
    "                stop_loss_min, stop_loss_max = combined_data_df.loc[combined_data_df['Indicator'] == indicator, ['Stop Loss Min', 'Stop Loss Max']].values[0]\n",
    "                target_hit_min, target_hit_max = combined_data_df.loc[combined_data_df['Indicator'] == indicator, ['Target Hit Min', 'Target Hit Max']].values[0]\n",
    "                \n",
    "                if pd.notna(stop_loss_min) and pd.notna(stop_loss_max):\n",
    "                    if pd.notna(target_hit_min) and pd.notna(target_hit_max):\n",
    "                        if stop_loss_min <= target_hit_min <= stop_loss_max:\n",
    "                            stop_loss_max = target_hit_min - 1e-9\n",
    "                        if stop_loss_min <= target_hit_max <= stop_loss_max:\n",
    "                            stop_loss_min = target_hit_max + 1e-9\n",
    "                    \n",
    "                    ranges_to_avoid[indicator] = (stop_loss_min, stop_loss_max)\n",
    "        \n",
    "            # Visualize the ranges to avoid\n",
    "            for indicator, avoid_range in ranges_to_avoid.items():\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                sns.histplot(stop_loss_hits[indicator], color='red', label='Stop Loss Hit', kde=True, stat=\"density\", linewidth=0)\n",
    "                sns.histplot(target_hits[indicator], color='green', label='Target Hit', kde=True, stat=\"density\", linewidth=0)\n",
    "                plt.axvspan(*avoid_range, color='red', alpha=0.3, label='Range to Avoid')\n",
    "                plt.xlabel(indicator)\n",
    "                plt.ylabel('Density')\n",
    "                plt.title(f'Distribution of {indicator} with Range to Avoid')\n",
    "                plt.legend()\n",
    "                plt.show()\n",
    "                plt.savefig(f'{output_dir}/{folder_name}_{indicator}_range_to_avoid.png')\n",
    "                plt.close()\n",
    "        \n",
    "            # Save the ranges to avoid in a text file\n",
    "            with open(f'{output_dir}/{folder_name}_ranges_to_avoid.txt', 'w') as f:\n",
    "                for indicator, (min_val, max_val) in ranges_to_avoid.items():\n",
    "                    f.write(f\"{indicator}: {min_val} to {max_val}\\n\")\n",
    "        \n",
    "            return ranges_to_avoid\n",
    "        \n",
    "        # Call the function to identify ranges to avoid\n",
    "        ranges_to_avoid = identify_ranges_to_avoid()\n",
    "\n",
    "        # Filter out trades that fall into the ranges to avoid\n",
    "        def filter_trades(trades, ranges_to_avoid):\n",
    "            for indicator, (min_val, max_val) in ranges_to_avoid.items():\n",
    "                if min_val is not None and max_val is not None:\n",
    "                    # Create a boolean mask for trades to remove\n",
    "                    mask = (trades[indicator] >= min_val) & (trades[indicator] <= max_val)\n",
    "                    trades = trades[~mask]\n",
    "            return trades\n",
    "\n",
    "        # Re-run the analysis excluding trades that fall into the ranges to avoid\n",
    "        new_trade_results_combined = []\n",
    "        new_all_combined_results = []\n",
    "        directories = [f'June 2024/{folder_name}']\n",
    "        for directory in directories:\n",
    "            folder_name = os.path.basename(directory)\n",
    "            for subdir, _, files in os.walk(directory):\n",
    "                if subdir == directory:\n",
    "                    continue  # Skip the top-level directory itself\n",
    "                for filename in files:\n",
    "                    if ', 1.csv' in filename:  # Change to ', 1.csv' for 1-minute data\n",
    "                        minute_1_path = os.path.join(subdir, filename)\n",
    "                        \n",
    "                        if os.path.exists(minute_1_path):\n",
    "                            print(f\"Re-processing {filename} in {folder_name}...\")\n",
    "                            \n",
    "                            # Load the CSV file\n",
    "                            minute_data_1 = pd.read_csv(minute_1_path)\n",
    "                            \n",
    "                            # Filter buy signals\n",
    "                            buy_signals = filter_signals(minute_data_1)\n",
    "                            \n",
    "                            # Skip if no buy signals\n",
    "                            if buy_signals.empty:\n",
    "                                print(f\"No buy signals found for {filename}. Skipping...\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Filter signals to only include those within the expiry week\n",
    "                            buy_signals = buy_signals[buy_signals['time'].apply(lambda x: is_within_expiry_week(expiry_date, x, folder_name))]\n",
    "                            \n",
    "                            # Skip if no buy signals within expiry week\n",
    "                            if buy_signals.empty:\n",
    "                                print(f\"No buy signals found within expiry week for {filename}. Skipping...\")\n",
    "                                continue\n",
    "                            \n",
    "                            # Perform the analysis\n",
    "                            analysis_results, trade_results = analyze_profit_and_stop_loss(buy_signals, minute_data_1, profit_percentages, stop_loss_percentages)\n",
    "                            \n",
    "                            # Filter out trades that fall into the ranges to avoid\n",
    "                            trade_results = filter_trades(trade_results, ranges_to_avoid)\n",
    "                            \n",
    "                            if not trade_results.empty:\n",
    "                                # Merge buy signals with trade results\n",
    "                                combined_results = pd.merge(buy_signals, trade_results, left_on='time', right_on='signal_time', how='inner')\n",
    "                                \n",
    "                                # Merge the 1-minute chart data with combined results\n",
    "                                combined_results = pd.merge(combined_results, minute_data_1, left_on='time', right_on='time', suffixes=('_buy', '_original'))\n",
    "                               \n",
    "                                new_all_combined_results.append(combined_results)\n",
    "                                new_trade_results_combined.append(trade_results)\n",
    "        \n",
    "        # Combine all results for the new filtered trades\n",
    "        if new_trade_results_combined:\n",
    "            new_combined_trade_results = pd.concat(new_trade_results_combined, ignore_index=True)\n",
    "            new_combined_all_results = pd.concat(new_all_combined_results, ignore_index=True)\n",
    "            \n",
    "            # Save the new filtered trade results to CSV files\n",
    "            new_output_dir = f'Outputs/DayTrade/SOSI/1min/{folder_name}_filtered'\n",
    "            os.makedirs(new_output_dir, exist_ok=True)\n",
    "            \n",
    "            new_combined_trade_results.to_csv(f'{new_output_dir}/{folder_name}_filtered_trade_results.csv', index=False)\n",
    "            new_combined_all_results.to_csv(f'{new_output_dir}/{folder_name}_filtered_all_combined_results.csv', index=False)\n",
    "            \n",
    "            # Re-calculate top 40 combinations for the filtered trades\n",
    "            new_combined_pnl_results = new_combined_trade_results.groupby(['Target Profit', 'Stop Loss'], as_index=False)['profit'].sum()\n",
    "            new_top_40_combinations = new_combined_pnl_results.sort_values(by='profit', ascending=False).head(40)\n",
    "            new_top_40_combinations.to_csv(f'{new_output_dir}/{folder_name}_filtered_top_40_combinations.csv', index=False)\n",
    "            \n",
    "            # Find the best combination of profit target and stop loss after filtering\n",
    "            best_combination_filtered = new_top_40_combinations.loc[new_top_40_combinations['profit'].idxmax()]\n",
    "            \n",
    "            # Filter the best trade results from the combined trade results\n",
    "            best_trade_results_filtered = new_combined_trade_results[\n",
    "                (new_combined_trade_results['Target Profit'] == best_combination_filtered['Target Profit']) & \n",
    "                (new_combined_trade_results['Stop Loss'] == best_combination_filtered['Stop Loss'])\n",
    "            ]\n",
    "            \n",
    "            # Re-summarize the best trade results after filtering\n",
    "            best_trade_summary_filtered = summarize_trade_results(best_trade_results_filtered)\n",
    "            \n",
    "            # Save the best trade summary after filtering\n",
    "            best_trade_summary_filtered_df = pd.DataFrame([best_trade_summary_filtered])\n",
    "            best_trade_summary_filtered_df.to_csv(f'{new_output_dir}/{folder_name}_filtered_best_trade_summary.csv', index=False)\n",
    "            \n",
    "            # Plot the new pie chart for best trade summary\n",
    "            labels = ['Target Hit', 'Stop Loss Hit', 'End of Day Exit']\n",
    "            sizes = [best_trade_summary_filtered['Target Hit'], best_trade_summary_filtered['Stop Loss Hit'], best_trade_summary_filtered['End of Day Exit']]\n",
    "            colors = ['lightgreen', 'lightcoral', 'lightskyblue']\n",
    "            plt.figure(figsize=(8, 8))\n",
    "            plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)\n",
    "            plt.title(f'Best Trade Summary for Best PNL Combination\\n{folder_name} (Filtered)')\n",
    "            plt.axis('equal')\n",
    "            plt.show()\n",
    "            plt.savefig(f'{new_output_dir}/{folder_name}_filtered_best_trade_summary_pie_chart.png')\n",
    "            plt.close()\n",
    "            \n",
    "            # Plot the new bar chart for total trades for best PNL\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(['Total Trades'], [best_trade_summary_filtered['Total Trades']], color='skyblue')\n",
    "            plt.xlabel('Count')\n",
    "            plt.ylabel('Total Trades')\n",
    "            plt.title(f'Total Trades for Best PNL Combination\\n{folder_name} (Filtered)')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig(f'{new_output_dir}/{folder_name}_filtered_total_trades_bar_chart.png')\n",
    "            plt.close()\n",
    "        \n",
    "            # Plot the new bar chart for max drawdown for best PNL\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(['Max Drawdown'], [best_trade_summary_filtered['Max Drawdown']], color='red')\n",
    "            plt.xlabel('Max Drawdown')\n",
    "            plt.ylabel('Max Drawdown Value')\n",
    "            plt.title(f'Max Drawdown for Best PNL Combination\\n{folder_name} (Filtered)')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig(f'{new_output_dir}/{folder_name}_filtered_max_drawdown_bar_chart.png')\n",
    "            plt.close()\n",
    "        \n",
    "            # Plot the bar chart for the top 40 combinations\n",
    "            plt.figure(figsize=(12, 8))\n",
    "            plt.barh(new_top_40_combinations['Target Profit'].astype(str) + \" / \" + new_top_40_combinations['Stop Loss'].astype(str),\n",
    "                     new_top_40_combinations['profit'], color='skyblue')\n",
    "            plt.xlabel('Total Profit')\n",
    "            plt.ylabel('Target Profit / Stop Loss')\n",
    "            plt.title(f'Top 40 Profit Target and Stop Loss Combinations\\n{folder_name} (Filtered)')\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.savefig(f'{new_output_dir}/{folder_name}_filtered_top_40_combinations.png')\n",
    "            plt.close()\n",
    "        \n",
    "        else:\n",
    "            print(f\"No trades remaining after filtering for {folder_name}.\")\n",
    "  \n",
    "print(\"Analysis complete.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
