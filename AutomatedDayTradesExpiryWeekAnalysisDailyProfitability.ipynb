{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing NSE_NIFTY240620C23400, 15.csv in NIFTY...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to filter buy signals from the 15-minute chart data\n",
    "def filter_signals(data):\n",
    "    buy_signals = data[(data['Original Buy Signal'] == True) & (data['Short Signal'] == False)]\n",
    "    return buy_signals\n",
    "\n",
    "# Function to simulate day trades\n",
    "def simulate_day_trades(buy_signals, minute_data, profit_target=0.02, stop_loss=0.01, min_stop_loss=0.005):\n",
    "    results = []\n",
    "\n",
    "    for index, buy_signal in buy_signals.iterrows():\n",
    "        entry_price = buy_signal['close']\n",
    "        entry_time = buy_signal['time'] + 900  # Add 15 minutes to the buy signal time to get the entry time\n",
    "        profit_price = entry_price * (1 + profit_target)\n",
    "        stop_price = entry_price * (1 - max(stop_loss, min_stop_loss))  # Ensure stop loss is at least min_stop_loss\n",
    "        \n",
    "        trade_result = {\n",
    "            'signal_time': buy_signal['time'],\n",
    "            'entry_time': entry_time,\n",
    "            'entry_price': entry_price,\n",
    "            'exit_time': None,\n",
    "            'exit_price': None,\n",
    "            'profit': None\n",
    "        }\n",
    "        \n",
    "        # Filter subsequent data for the same day only\n",
    "        trade_date = pd.to_datetime(entry_time, unit='s').date()\n",
    "        subsequent_data = minute_data[(minute_data['time'] > entry_time) & \n",
    "                                      (pd.to_datetime(minute_data['time'], unit='s').dt.date == trade_date)]\n",
    "        \n",
    "        for _, row in subsequent_data.iterrows():\n",
    "            if row['high'] >= profit_price:\n",
    "                trade_result['exit_time'] = row['time']\n",
    "                trade_result['exit_price'] = profit_price\n",
    "                trade_result['profit'] = profit_price - entry_price\n",
    "                break\n",
    "            if row['low'] <= stop_price:\n",
    "                trade_result['exit_time'] = row['time']\n",
    "                trade_result['exit_price'] = stop_price\n",
    "                trade_result['profit'] = stop_price - entry_price\n",
    "                break\n",
    "        else:\n",
    "            # If no target or stop loss hit, exit at the last price of the day\n",
    "            if not subsequent_data.empty:\n",
    "                last_row = subsequent_data.iloc[-1]\n",
    "                trade_result['exit_time'] = last_row['time']\n",
    "                trade_result['exit_price'] = last_row['close']\n",
    "                trade_result['profit'] = last_row['close'] - entry_price\n",
    "        \n",
    "        results.append(trade_result)\n",
    "    \n",
    "    return pd.DataFrame(results).dropna(subset=['entry_price'])\n",
    "\n",
    "# Function to analyze profit and stop loss percentages for each day\n",
    "def analyze_profit_and_stop_loss_daily(buy_signals, minute_data, profit_percentages, stop_loss_percentages, min_stop_loss=0.005):\n",
    "    daily_analysis_results = []\n",
    "    combined_trade_results = []\n",
    "\n",
    "    buy_signals['date'] = pd.to_datetime(buy_signals['time'], unit='s').dt.date\n",
    "\n",
    "    for trade_date in buy_signals['date'].unique():\n",
    "        daily_signals = buy_signals[buy_signals['date'] == trade_date]\n",
    "        daily_trade_results = []\n",
    "\n",
    "        for profit_target in profit_percentages:\n",
    "            for stop_loss in stop_loss_percentages:\n",
    "                trade_results = simulate_day_trades(daily_signals, minute_data, profit_target, stop_loss, min_stop_loss)\n",
    "                total_profit = trade_results['profit'].sum()\n",
    "                daily_trade_results.append((trade_date, profit_target, stop_loss, total_profit))\n",
    "                combined_trade_results.append(trade_results)\n",
    "        \n",
    "        daily_analysis_results.append(pd.DataFrame(daily_trade_results, columns=['Date', 'Profit Target', 'Stop Loss', 'Total Profit']))\n",
    "\n",
    "    return pd.concat(daily_analysis_results, ignore_index=True), pd.concat(combined_trade_results, ignore_index=True)\n",
    "\n",
    "# Function to find the best daily combinations and average them\n",
    "def find_average_best_combination(daily_analysis_results):\n",
    "    best_combinations = daily_analysis_results.loc[daily_analysis_results.groupby('Date')['Total Profit'].idxmax()]\n",
    "    avg_profit_target = best_combinations['Profit Target'].mean()\n",
    "    avg_stop_loss = best_combinations['Stop Loss'].mean()\n",
    "    \n",
    "    return avg_profit_target, avg_stop_loss, best_combinations\n",
    "\n",
    "# Define profit percentages and stop loss percentages to analyze\n",
    "profit_percentages = [i/100 for i in range(1, 200)]\n",
    "stop_loss_percentages = [i/100 for i in range(1, 20)]\n",
    "\n",
    "# Directories containing the data\n",
    "directories = ['June 2024/NIFTY', 'June 2024/BANKNIFTY', 'June 2024/FINNIFTY', 'June 2024/SENSEX']  # Change the directories as needed\n",
    "\n",
    "# Function to check if the date is within the week of expiration\n",
    "def is_within_expiry_week(expiry_date, timestamp, folder_name, days_before_expiry=5):\n",
    "    # Assuming the date is the 7th to 12th characters in the string\n",
    "    if folder_name == 'NIFTY':\n",
    "        date_part = expiry_date[9:15]\n",
    "    if folder_name == 'BANKNIFTY':\n",
    "        date_part = expiry_date[13:19]\n",
    "    if folder_name == 'FINNIFTY':\n",
    "        date_part = expiry_date[12:18]\n",
    "    if folder_name == 'SENSEX':\n",
    "        date_part = expiry_date[7:13]\n",
    "    \n",
    "    expiry_datetime = pd.to_datetime(date_part, format='%y%m%d')\n",
    "    signal_datetime = pd.to_datetime(timestamp, unit='s')\n",
    "    return (expiry_datetime - signal_datetime).days <= days_before_expiry\n",
    "\n",
    "# Process each directory\n",
    "for directory in directories:\n",
    "    pnl_results = []\n",
    "    trade_results_combined = []\n",
    "    all_combined_results = []\n",
    "    folder_name = os.path.basename(directory)\n",
    "    for subdir, _, files in os.walk(directory):\n",
    "        if subdir == directory:\n",
    "            continue  # Skip the top-level directory itself\n",
    "        for filename in files:\n",
    "            if ', 15.csv' in filename:\n",
    "                minute_15_path = os.path.join(subdir, filename)\n",
    "                minute_1_path = minute_15_path.replace(', 15.csv', ', 1.csv')\n",
    "                \n",
    "                if os.path.exists(minute_1_path):\n",
    "                    print(f\"Processing {filename} in {folder_name}...\")\n",
    "                    \n",
    "                    # Extract expiry date from filename\n",
    "                    expiry_date = filename.split(' ')[0].split(',')[0]\n",
    "                    \n",
    "                    # Load the CSV files\n",
    "                    minute_data_15 = pd.read_csv(minute_15_path)\n",
    "                    minute_data_1 = pd.read_csv(minute_1_path)\n",
    "                    \n",
    "                    # Filter buy signals\n",
    "                    buy_signals = filter_signals(minute_data_15)\n",
    "                    \n",
    "                    # Filter signals to only include those within the expiry week\n",
    "                    buy_signals = buy_signals[buy_signals['time'].apply(lambda x: is_within_expiry_week(expiry_date, x, folder_name))]\n",
    "                    \n",
    "                    # Skip if no buy signals\n",
    "                    if buy_signals.empty:\n",
    "                        print(f\"No buy signals found within expiry week for {filename}. Skipping...\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Perform the analysis\n",
    "                    daily_analysis_results, trade_results = analyze_profit_and_stop_loss_daily(buy_signals, minute_data_1, profit_percentages, stop_loss_percentages)\n",
    "                    \n",
    "                    # Merge buy signals with trade results\n",
    "                    combined_results = pd.merge(buy_signals, trade_results, left_on='time', right_on='signal_time', how='inner')\n",
    "                    \n",
    "                    # Merge the 15-minute chart data with combined results\n",
    "                    combined_results = pd.merge(combined_results, minute_data_15, left_on='time', right_on='time', suffixes=('_buy', '_original'))\n",
    "                   \n",
    "                    all_combined_results.append(combined_results)\n",
    "                    \n",
    "                    # Append to combined results\n",
    "                    pnl_results.append(daily_analysis_results)\n",
    "                    trade_results_combined.append(trade_results)\n",
    "    \n",
    "    # Combine all results for the folder\n",
    "    combined_pnl_results = pd.concat(pnl_results, ignore_index=True)\n",
    "    # Aggregate total profits for the same combinations\n",
    "    combined_pnl_results = combined_pnl_results.groupby(['Date', 'Profit Target', 'Stop Loss'], as_index=False)['Total Profit'].sum()\n",
    "    combined_trade_results = pd.concat(trade_results_combined, ignore_index=True)\n",
    "    combined_all_results = pd.concat(all_combined_results, ignore_index=True)\n",
    "    \n",
    "    # Save the results to CSV files\n",
    "    combined_pnl_results.to_csv(f'Outputs/DayTrade/HumanRelatable/{folder_name}_daily_pnl_analysis.csv', index=False)\n",
    "    combined_trade_results.to_csv(f'Outputs/DayTrade/HumanRelatable/{folder_name}_daily_trade_results.csv', index=False)\n",
    "    combined_all_results.to_csv(f'Outputs/DayTrade/HumanRelatable/{folder_name}_daily_all_combined_results.csv', index=False)\n",
    "    \n",
    "    # Find the best combination of profit target and stop loss for each day\n",
    "    best_combinations = combined_pnl_results.loc[combined_pnl_results.groupby('Date')['Total Profit'].idxmax()]\n",
    "    \n",
    "    # Save the best combinations\n",
    "    best_combinations.to_csv(f'Outputs/DayTrade/HumanRelatable/{folder_name}_daily_best_combinations.csv', index=False)\n",
    "    \n",
    "    # Save the top 40 combinations\n",
    "    top_40_combinations = combined_pnl_results.sort_values(by='Total Profit', ascending=False).head(40)\n",
    "    top_40_combinations.to_csv(f'Outputs/DayTrade/HumanRelatable/{folder_name}_top_40_combinations.csv', index=False)\n",
    "\n",
    "    # Find the average best combination\n",
    "    avg_profit_target, avg_stop_loss, best_combinations = find_average_best_combination(best_combinations)\n",
    "    \n",
    "    # Ensure the average combination is profitable on a daily basis\n",
    "    def is_profitable_daily(buy_signals, minute_data, avg_profit_target, avg_stop_loss, min_stop_loss=0.005):\n",
    "        daily_results = []\n",
    "        buy_signals['date'] = pd.to_datetime(buy_signals['time'], unit='s').dt.date\n",
    "    \n",
    "        for trade_date in buy_signals['date'].unique():\n",
    "            daily_signals = buy_signals[buy_signals['date'] == trade_date]\n",
    "            trade_results = simulate_day_trades(daily_signals, minute_data, avg_profit_target, avg_stop_loss, min_stop_loss)\n",
    "            total_profit = trade_results['profit'].sum()\n",
    "            daily_results.append(total_profit)\n",
    "        \n",
    "        return all(profit > 0 for profit in daily_results), daily_results\n",
    "    \n",
    "    # Check profitability of average combination\n",
    "    profitable, daily_profits = is_profitable_daily(buy_signals, minute_data_1, avg_profit_target, avg_stop_loss)\n",
    "    \n",
    "    if profitable:\n",
    "        print(f\"The average combination (Profit Target: {avg_profit_target}, Stop Loss: {avg_stop_loss}) is profitable on a daily basis.\")\n",
    "    else:\n",
    "        print(f\"The average combination (Profit Target: {avg_profit_target}, Stop Loss: {avg_stop_loss}) is not profitable on a daily basis.\")\n",
    "    print(daily_results)\n",
    "    # Save the average best combination\n",
    "    avg_best_combination_df = pd.DataFrame({\n",
    "        'Profit Target': [avg_profit_target],\n",
    "        'Stop Loss': [avg_stop_loss]\n",
    "    })\n",
    "    avg_best_combination_df.to_csv(f'Outputs/DayTrade/HumanRelatable/{folder_name}_avg_best_combination.csv', index=False)\n",
    "    \n",
    "    # Save the top 40 best daily average profit stop-loss combinations\n",
    "    daily_best_combinations = combined_pnl_results.groupby(['Profit Target', 'Stop Loss'], as_index=False)['Total Profit'].mean()\n",
    "    top_40_daily_best_combinations = daily_best_combinations.sort_values(by='Total Profit', ascending=False).head(40)\n",
    "    top_40_daily_best_combinations.to_csv(f'Outputs/DayTrade/HumanRelatable/{folder_name}_top_40_daily_best_combinations.csv', index=False)\n",
    "    \n",
    "    # Plot the heatmap for better visualization\n",
    "    for trade_date in combined_pnl_results['Date'].unique():\n",
    "        daily_results = combined_pnl_results[combined_pnl_results['Date'] == trade_date]\n",
    "        pivot_table = daily_results.pivot(index=\"Stop Loss\", columns=\"Profit Target\", values=\"Total Profit\")\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        sns.heatmap(pivot_table, annot=True, fmt=\".2f\", cmap=\"coolwarm\", cbar_kws={'label': 'Total Profit'})\n",
    "        plt.title(f\"Total Profit for Different Profit Targets and Stop Loss Percentages\\n{folder_name} - {trade_date}\")\n",
    "        plt.xlabel(\"Profit Target\")\n",
    "        plt.ylabel(\"Stop Loss\")\n",
    "        plt.show()\n",
    "        plt.savefig(f'Outputs/DayTrade/HumanRelatable/{folder_name}_{trade_date}_heatmap.png')\n",
    "        plt.close()\n",
    "    \n",
    "    # Plot the bar chart for top 40 combinations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(top_40_combinations['Profit Target'].astype(str) + \" / \" + top_40_combinations['Stop Loss'].astype(str),\n",
    "             top_40_combinations['Total Profit'], color='skyblue')\n",
    "    plt.xlabel('Total Profit')\n",
    "    plt.ylabel('Profit Target / Stop Loss')\n",
    "    plt.title(f'Top 40 Profit Target and Stop Loss Combinations\\n{folder_name}')\n",
    "    plt.gca().invert_yaxis()  # To display the highest\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f'Outputs/DayTrade/HumanRelatable/{folder_name}_top_40_combinations.png')\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot the bar chart for top 40 daily best average combinations\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plt.barh(top_40_daily_best_combinations['Profit Target'].astype(str) + \" / \" + top_40_daily_best_combinations['Stop Loss'].astype(str),\n",
    "             top_40_daily_best_combinations['Total Profit'], color='lightcoral')\n",
    "    plt.xlabel('Total Profit')\n",
    "    plt.ylabel('Profit Target / Stop Loss')\n",
    "    plt.title(f'Top 40 Daily Best Average Profit Target and Stop Loss Combinations\\n{folder_name}')\n",
    "    plt.gca().invert_yaxis()  # To display the highest\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.savefig(f'Outputs/DayTrade/HumanRelatable/{folder_name}_top_40_daily_best_combinations.png')\n",
    "    plt.close()\n",
    "\n",
    "print(\"Analysis complete.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, the best target profit and stop loss combination for the whole expiry week is very robotic and out of human emotion, so we try to get daily average profitability such that its according to human emotion and sustainable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
